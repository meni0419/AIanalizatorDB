#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import json
import time
import threading
from pathlib import Path


class DeepSeekAPIChat:
    def __init__(self, model_name="deepseek-r1:8b", max_context_tokens=128000, ollama_url="http://localhost:11434"):
        self.model_name = model_name
        self.ollama_url = ollama_url
        self.conversation_history = []
        self.max_context_tokens = max_context_tokens
        self.model_loaded = False
        self.lock = threading.Lock()

    def check_ollama_connection(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception:
            return False

    def check_model_loaded(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å"""
        try:
            response = requests.get(f"{self.ollama_url}/api/ps", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = data.get('models', [])
                for model in models:
                    if model.get('name') == self.model_name:
                        self.model_loaded = True
                        return True
            self.model_loaded = False
            return False
        except Exception:
            self.model_loaded = False
            return False

    def preload_model(self):
        """–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å"""
        print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å {self.model_name} –≤ –ø–∞–º—è—Ç—å...")
        print("‚è±Ô∏è  –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π...")

        try:
            start_time = time.time()

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            payload = {
                "model": self.model_name,
                "prompt": "–ü—Ä–∏–≤–µ—Ç! –°–∫–∞–∂–∏ —Ç–æ–ª—å–∫–æ '–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!'",
                "stream": False,
                "keep_alive": "24h",  # –î–µ—Ä–∂–∏–º –º–æ–¥–µ–ª—å 24 —á–∞—Å–∞
                "options": {
                    "temperature": 0.1,
                    "num_ctx": 131072
                }
            }

            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=300  # 5 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
            )

            if response.status_code == 200:
                data = response.json()
                load_time = time.time() - start_time

                print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥")
                print(f"ü§ñ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {data.get('response', '–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞')}")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
                if self.check_model_loaded():
                    print("‚úÖ –ú–æ–¥–µ–ª—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ –≤ —Å–ø–∏—Å–∫–µ –∑–∞–ø—É—â–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤")
                    return True
                else:
                    print("‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–æ –Ω–µ –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –≤ ollama ps")
                    self.model_loaded = True  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Ä—É—á–Ω—É—é
                    return True
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {response.status_code}")
                print(f"–û—Ç–≤–µ—Ç: {response.text}")
                return False

        except requests.exceptions.Timeout:
            print("‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ (5 –º–∏–Ω—É—Ç)")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
            return False

    def keep_model_alive(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–∏–Ω–≥—É–µ—Ç –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã –æ–Ω–∞ –Ω–µ –≤—ã–≥—Ä—É–∂–∞–ª–∞—Å—å"""
        try:
            payload = {
                "model": self.model_name,
                "prompt": "ping",
                "stream": False,
                "keep_alive": "24h"
            }

            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=30
            )

            return response.status_code == 200
        except Exception:
            return False

    def estimate_tokens(self, text):
        """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        return len(text) // 4

    def get_context_size(self):
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö"""
        total_tokens = 0
        for msg in self.conversation_history:
            total_tokens += self.estimate_tokens(msg["content"])
        return total_tokens

    def manage_context(self):
        """–£–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–∞–∑–º–µ—Ä–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        current_tokens = self.get_context_size()
        max_history_tokens = int(self.max_context_tokens * 0.8)

        if current_tokens > max_history_tokens:
            print(f"‚ö†Ô∏è  –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω ({current_tokens} —Ç–æ–∫–µ–Ω–æ–≤). –£–¥–∞–ª—è—é —Å—Ç–∞—Ä—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è...")

            while len(self.conversation_history) > 2 and self.get_context_size() > max_history_tokens:
                self.conversation_history.pop(0)
                self.conversation_history.pop(0)

            print(f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–∂–∞—Ç –¥–æ {self.get_context_size()} —Ç–æ–∫–µ–Ω–æ–≤")

        return current_tokens

    def load_file_content(self, file_path):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞"""
        try:
            path = Path(file_path)
            if not path.exists():
                return f"[–û–®–ò–ë–ö–ê: –§–∞–π–ª '{file_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω]"

            if not path.is_file():
                return f"[–û–®–ò–ë–ö–ê: '{file_path}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ–∞–π–ª–æ–º]"

            max_file_size = 5 * 1024 * 1024  # 5MB
            if path.stat().st_size > max_file_size:
                return f"[–û–®–ò–ë–ö–ê: –§–∞–π–ª '{file_path}' —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π (>5MB)]"

            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()

            content_tokens = self.estimate_tokens(content)
            if content_tokens > self.max_context_tokens * 0.6:
                return f"[–û–®–ò–ë–ö–ê: –§–∞–π–ª '{file_path}' —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({content_tokens} —Ç–æ–∫–µ–Ω–æ–≤)]"

            file_info = f"[–§–∞–π–ª: {file_path}]\n"
            file_info += f"[–†–∞–∑–º–µ—Ä: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤, ~{content_tokens} —Ç–æ–∫–µ–Ω–æ–≤]\n"
            file_info += "--- –°–û–î–ï–†–ñ–ò–ú–û–ï –§–ê–ô–õ–ê ---\n"
            file_info += content
            file_info += "\n--- –ö–û–ù–ï–¶ –§–ê–ô–õ–ê ---"

            return file_info

        except UnicodeDecodeError:
            return f"[–û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª '{file_path}' - –≤–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –±–∏–Ω–∞—Ä–Ω—ã–π —Ñ–∞–π–ª]"
        except Exception as e:
            return f"[–û–®–ò–ë–ö–ê –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ '{file_path}': {str(e)}]"

    def process_file_references(self, text):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª—ã –≤ —Ç–µ–∫—Å—Ç–µ"""
        import re
        pattern = r'#file:([^\s]+)'

        def replace_file_ref(match):
            file_path = match.group(1)
            return self.load_file_content(file_path)

        processed_text = re.sub(pattern, replace_file_ref, text)
        return processed_text

    def send_message(self, message):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ DeepSeek —á–µ—Ä–µ–∑ API"""
        with self.lock:
            if not self.model_loaded:
                return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ preload_model()"}

            try:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª—ã
                processed_message = self.process_file_references(message)

                # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
                self.conversation_history.append({
                    "role": "user",
                    "content": processed_message
                })

                # –£–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
                self.manage_context()

                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –∏—Å—Ç–æ—Ä–∏–µ–π
                full_prompt = ""
                for msg in self.conversation_history:
                    role = "Human" if msg["role"] == "user" else "Assistant"
                    full_prompt += f"{role}: {msg['content']}\n\n"

                full_prompt += "Assistant: "

                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
                start_time = time.time()

                payload = {
                    "model": self.model_name,
                    "prompt": full_prompt,
                    "stream": False,
                    "keep_alive": "24h",
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "num_ctx": 131072
                    }
                }

                response = requests.post(
                    f"{self.ollama_url}/api/generate",
                    json=payload,
                    timeout=300
                )

                response_time = time.time() - start_time

                if response.status_code == 200:
                    data = response.json()
                    assistant_response = data.get('response', '–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞')

                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
                    self.conversation_history.append({
                        "role": "assistant",
                        "content": assistant_response
                    })

                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º thinking –±–ª–æ–∫–∏ –¥–ª—è DeepSeek-R1
                    thinking_text = ""
                    final_response = assistant_response

                    if "<thinking>" in assistant_response and "</thinking>" in assistant_response:
                        import re
                        thinking_match = re.search(r'<thinking>(.*?)</thinking>', assistant_response, re.DOTALL)
                        if thinking_match:
                            thinking_text = thinking_match.group(1).strip()
                            final_response = re.sub(r'<thinking>.*?</thinking>', '', assistant_response,
                                                    flags=re.DOTALL).strip()

                    return {
                        "success": True,
                        "thinking": thinking_text,
                        "response": final_response,
                        "response_time": round(response_time, 2),
                        "tokens_used": data.get('eval_count', 0),
                        "tokens_per_second": round(data.get('eval_count', 0) / response_time,
                                                   2) if response_time > 0 else 0
                    }
                else:
                    return {"error": f"–û—à–∏–±–∫–∞ API: {response.status_code} - {response.text}"}

            except requests.exceptions.Timeout:
                return {"error": "–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ (5 –º–∏–Ω—É—Ç)"}
            except Exception as e:
                return {"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {str(e)}"}

    def unload_model(self):
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏"""
        try:
            print("üîÑ –í—ã–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏...")

            payload = {
                "model": self.model_name,
                "keep_alive": 0  # –ù–µ–º–µ–¥–ª–µ–Ω–Ω–∞—è –≤—ã–≥—Ä—É–∑–∫–∞
            }

            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=30
            )

            self.model_loaded = False
            print("‚úÖ –ú–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ø–∞–º—è—Ç–∏")
            return True

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
            return False

    def clear_history(self):
        """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        self.conversation_history = []
        print("‚úÖ –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –æ—á–∏—â–µ–Ω–∞")

    def get_status(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã"""
        current_tokens = self.get_context_size()
        used_percent = (current_tokens / self.max_context_tokens) * 100

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–∏
        model_status = self.check_model_loaded()

        return {
            "model_loaded": model_status,
            "model_name": self.model_name,
            "current_tokens": current_tokens,
            "max_tokens": self.max_context_tokens,
            "used_percent": round(used_percent, 1),
            "messages_count": len(self.conversation_history),
            "ollama_connected": self.check_ollama_connection()
        }


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è CLI –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è CLI"""
    model_name = "deepseek-r1:8b"

    chat = DeepSeekAPIChat(model_name)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama
    if not chat.check_ollama_connection():
        print("‚ùå Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω! –ó–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve")
        return

    print("üöÄ DeepSeek-R1 API Chat –∑–∞–ø—É—â–µ–Ω!")
    print("üí¨ –ö–æ–º–∞–Ω–¥—ã: /preload, /status, /clear, /exit")
    print("-" * 50)

    while True:
        try:
            user_input = input("\nüë§ –í—ã: ").strip()

            if not user_input:
                continue

            if user_input.lower() == "/exit":
                if chat.model_loaded:
                    chat.unload_model()
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            elif user_input.lower() == "/preload":
                chat.preload_model()
                continue
            elif user_input.lower() == "/status":
                status = chat.get_status()
                print(f"\nüìä –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã:")
                print(f"  –ú–æ–¥–µ–ª—å: {status['model_name']}")
                print(f"  –°—Ç–∞—Ç—É—Å: {'üü¢ –ó–∞–≥—Ä—É–∂–µ–Ω–∞' if status['model_loaded'] else 'üî¥ –ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'}")
                print(f"  Ollama: {'üü¢ –ü–æ–¥–∫–ª—é—á–µ–Ω' if status['ollama_connected'] else 'üî¥ –ù–µ –ø–æ–¥–∫–ª—é—á–µ–Ω'}")
                print(f"  –¢–æ–∫–µ–Ω—ã: {status['current_tokens']:,} / {status['max_tokens']:,} ({status['used_percent']}%)")
                print(f"  –°–æ–æ–±—â–µ–Ω–∏–π: {status['messages_count']}")
                continue
            elif user_input.lower() == "/clear":
                chat.clear_history()
                continue

            if not chat.model_loaded:
                print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /preload")
                continue

            print("ü§ñ DeepSeek –¥—É–º–∞–µ—Ç...")
            result = chat.send_message(user_input)

            if result.get("success"):
                if result.get("thinking"):
                    print(f"\nü§î –†–∞–∑–º—ã—à–ª–µ–Ω–∏—è: {result['thinking']}")
                print(f"\nü§ñ AI: {result['response']}")
                print(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {result['response_time']}—Å | –¢–æ–∫–µ–Ω—ã/—Å–µ–∫: {result['tokens_per_second']}")
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞: {result.get('error')}")

        except KeyboardInterrupt:
            print("\n\nüîÑ –í—ã–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥ –≤—ã—Ö–æ–¥–æ–º...")
            if chat.model_loaded:
                chat.unload_model()
            print("üëã –ß–∞—Ç –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")


if __name__ == "__main__":
    main()